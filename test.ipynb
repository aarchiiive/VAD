{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f6ef58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68160218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1310"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = Path('/home/work/datasets/navsim_full/navsim/dataset/navsim_logs/trainval')\n",
    "logs = list(log_dir.glob('*.pkl'))\n",
    "len(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e144404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_log = pickle.load(open(logs[0], 'rb'))\n",
    "len(sample_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e18bdc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token', 'frame_idx', 'timestamp', 'log_name', 'log_token', 'scene_name', 'scene_token', 'map_location', 'roadblock_ids', 'vehicle_name', 'can_bus', 'lidar_path', 'lidar2ego_translation', 'lidar2ego_rotation', 'ego2global_translation', 'ego2global_rotation', 'ego_dynamic_state', 'traffic_lights', 'driving_command', 'cams', 'sample_prev', 'sample_next', 'ego2global', 'lidar2ego', 'lidar2global', 'anns', 'occ_gt_final_path', 'flow_gt_final_path'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_log[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545d6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = \"/home/work/datasets/navsim_full/navsim/dataset\"\n",
    "os.environ[\"NUPLAN_MAP_VERSION\"] = \"nuplan-maps-v1.0\"\n",
    "os.environ[\"NUPLAN_MAPS_ROOT\"] = f\"{data_dir}/maps\"\n",
    "os.environ[\"NAVSIM_EXP_ROOT\"] = \"/home/work/song99/VAD/navsim/exp\"\n",
    "os.environ[\"NAVSIM_DEVKIT_ROOT\"] = \"/home/work/song99/VAD/navsim/\"\n",
    "os.environ[\"OPENSCENE_DATA_ROOT\"] = f\"{data_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f452ff6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth\" to /home/work/.cache/torch/hub/checkpoints/resnet34-43635321.pth\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize_config_module, compose\n",
    "from hydra.utils import instantiate\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from navsim.agents.abstract_agent import AbstractAgent\n",
    "from navsim.common.dataclasses import SceneFilter, SensorConfig\n",
    "from navsim.common.dataloader import SceneLoader\n",
    "from navsim.planning.training.dataset import Dataset\n",
    "from navsim.planning.training.dataset import CacheOnlyDataset\n",
    "\n",
    "split = \"navtrain\"\n",
    "\n",
    "# Compose the training config via Hydra\n",
    "GlobalHydra.instance().clear()\n",
    "with initialize_config_module(\n",
    "    config_module=\"navsim.planning.script.config.training\",\n",
    "    version_base=None,\n",
    "):\n",
    "    cfg = compose(\n",
    "        config_name=\"default_training\",\n",
    "        overrides=[\n",
    "            \"experiment_name=jupyter_debug\",\n",
    "            f\"train_test_split={split}\",\n",
    "            \"cache_path=/home/work/song99/VAD/cache\",\n",
    "            \"agent=transfuser_agent\",\n",
    "            # \"train_test_split.scene_filter.max_scenes=4\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# Build agent, datasets, and dataloaders\n",
    "agent = instantiate(cfg.agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28bc18e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading logs: 100%|██████████| 214/214 [00:25<00:00,  8.40it/s]\n"
     ]
    }
   ],
   "source": [
    "train_scene_filter: SceneFilter = instantiate(cfg.train_test_split.scene_filter)\n",
    "if train_scene_filter.log_names is not None:\n",
    "    train_scene_filter.log_names = [\n",
    "        log_name for log_name in train_scene_filter.log_names if log_name in cfg.train_logs\n",
    "    ]\n",
    "else:\n",
    "    train_scene_filter.log_names = cfg.train_logs\n",
    "\n",
    "val_scene_filter: SceneFilter = instantiate(cfg.train_test_split.scene_filter)\n",
    "if val_scene_filter.log_names is not None:\n",
    "    val_scene_filter.log_names = [\n",
    "        log_name for log_name in val_scene_filter.log_names if log_name in cfg.val_logs\n",
    "    ]\n",
    "else:\n",
    "    val_scene_filter.log_names = cfg.val_logs\n",
    "\n",
    "data_path = Path(cfg.navsim_log_path)\n",
    "original_sensor_path = Path(cfg.original_sensor_path)\n",
    "\n",
    "history_steps = [0, 1, 2, 3]\n",
    "sensor_config = SensorConfig(\n",
    "    cam_f0=history_steps,\n",
    "    cam_l0=history_steps,\n",
    "    cam_l1=history_steps,\n",
    "    cam_l2=history_steps,\n",
    "    cam_r0=history_steps,\n",
    "    cam_r1=history_steps,\n",
    "    cam_r2=history_steps,\n",
    "    cam_b0=history_steps,\n",
    "    lidar_pc=history_steps,\n",
    ")\n",
    "\n",
    "# train_scene_loader = SceneLoader(\n",
    "#     original_sensor_path=original_sensor_path,\n",
    "#     data_path=data_path,\n",
    "#     scene_filter=train_scene_filter,\n",
    "#     sensor_config=agent.get_sensor_config(),\n",
    "# )\n",
    "\n",
    "val_scene_loader = SceneLoader(\n",
    "    original_sensor_path=original_sensor_path,\n",
    "    data_path=data_path,\n",
    "    scene_filter=val_scene_filter,\n",
    "    sensor_config=sensor_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb7aba24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18179"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_scene_loader.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d5c5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "navsim_val_tokens = val_scene_loader.tokens\n",
    "\n",
    "with open('navsim_val_tokens.pkl', 'wb') as f:\n",
    "    pickle.dump(navsim_val_tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "476ab96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from enum import IntEnum\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "from nuplan.common.actor_state.oriented_box import OrientedBox\n",
    "from nuplan.common.actor_state.state_representation import StateSE2\n",
    "from nuplan.common.actor_state.tracked_objects_types import TrackedObjectType\n",
    "from nuplan.common.maps.abstract_map import AbstractMap, MapObject, SemanticMapLayer\n",
    "from nuplan.planning.simulation.trajectory.trajectory_sampling import TrajectorySampling\n",
    "from shapely import affinity\n",
    "from shapely.geometry import LineString, Polygon\n",
    "from torchvision import transforms\n",
    "\n",
    "from navsim.agents.vad.vad_config import VADConfig\n",
    "from navsim.common.dataclasses import AgentInput, Annotations, Scene\n",
    "from navsim.common.enums import BoundingBoxIndex, LidarIndex\n",
    "from navsim.planning.scenario_builder.navsim_scenario_utils import tracked_object_types\n",
    "from navsim.planning.training.abstract_feature_target_builder import AbstractFeatureBuilder, AbstractTargetBuilder\n",
    "\n",
    "\n",
    "class VADFeatureBuilder(AbstractFeatureBuilder):\n",
    "    \"\"\"Input feature builder for VAD.\"\"\"\n",
    "\n",
    "    def __init__(self, config: VADConfig):\n",
    "        \"\"\"\n",
    "        Initializes feature builder.\n",
    "        :param config: global config dataclass of VAD\n",
    "        \"\"\"\n",
    "        self._config = config\n",
    "        self.img2tensor = transforms.ToTensor()\n",
    "\n",
    "    def get_unique_name(self) -> str:\n",
    "        \"\"\"Inherited, see superclass.\"\"\"\n",
    "        return \"vad_feature\"\n",
    "\n",
    "    def compute_features(self, agent_input: AgentInput) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Inherited, see superclass.\"\"\"\n",
    "        features = {}\n",
    "        features[\"ego_status\"] = self._get_ego_status(agent_input)\n",
    "        # features[\"images\"] = self._get_images(agent_input)\n",
    "        features[\"image_paths\"] = self._get_image_paths(agent_input)\n",
    "        features[\"lidar_path\"] = self._get_lidar_path(agent_input)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _get_ego_status(self, agent_input: AgentInput) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract ego status from AgentInput\n",
    "        :param agent_input: input dataclass\n",
    "        :return: ego status as torch tensor\n",
    "\n",
    "        ego_pose: npt.NDArray[np.float64]\n",
    "        ego_velocity: npt.NDArray[np.float32]\n",
    "        ego_acceleration: npt.NDArray[np.float32]\n",
    "        driving_command: npt.NDArray[np.int]\n",
    "        in_global_frame: bool = False  # False for AgentInput\n",
    "\n",
    "        \"\"\"\n",
    "        ego_status = agent_input.ego_statuses[-1]\n",
    "        ego_pose = ego_status.ego_pose\n",
    "        ego_velocity = ego_status.ego_velocity\n",
    "        ego_acceleration = ego_status.ego_acceleration\n",
    "        driving_command = ego_status.driving_command\n",
    "\n",
    "        return {\n",
    "            \"ego_pose\": torch.tensor(ego_pose, dtype=torch.float32),\n",
    "            \"ego_velocity\": torch.tensor(ego_velocity, dtype=torch.float32),\n",
    "            \"ego_acceleration\": torch.tensor(ego_acceleration, dtype=torch.float32),\n",
    "            \"driving_command\": torch.tensor(driving_command, dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "    def _get_images(self, agent_input: AgentInput) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract images from AgentInput\n",
    "        :param agent_input: input dataclass\n",
    "        :return: front view image as torch tensor\n",
    "        \"\"\"\n",
    "        cameras = agent_input.cameras[-1]\n",
    "\n",
    "        return {\n",
    "            \"cam_f0\": self.img2tensor(cameras.cam_f0.image),\n",
    "            \"cam_l0\": self.img2tensor(cameras.cam_l0.image),\n",
    "            \"cam_l1\": self.img2tensor(cameras.cam_l1.image),\n",
    "            \"cam_l2\": self.img2tensor(cameras.cam_l2.image),\n",
    "            \"cam_r0\": self.img2tensor(cameras.cam_r0.image),\n",
    "            \"cam_r1\": self.img2tensor(cameras.cam_r1.image),\n",
    "            \"cam_r2\": self.img2tensor(cameras.cam_r2.image),\n",
    "            \"cam_b0\": self.img2tensor(cameras.cam_b0.image),\n",
    "        }\n",
    "\n",
    "    def _get_image_paths(self, agent_input: AgentInput) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Extract image paths from AgentInput\n",
    "        :param agent_input: input dataclass\n",
    "        :return: front view image path as string\n",
    "        \"\"\"\n",
    "        cameras = agent_input.cameras[-1]\n",
    "\n",
    "        return {\n",
    "            \"cam_f0\": cameras.cam_f0.camera_path,\n",
    "            \"cam_l0\": cameras.cam_l0.camera_path,\n",
    "            \"cam_l1\": cameras.cam_l1.camera_path,\n",
    "            \"cam_l2\": cameras.cam_l2.camera_path,\n",
    "            \"cam_r0\": cameras.cam_r0.camera_path,\n",
    "            \"cam_r1\": cameras.cam_r1.camera_path,\n",
    "            \"cam_r2\": cameras.cam_r2.camera_path,\n",
    "            \"cam_b0\": cameras.cam_b0.camera_path,\n",
    "        }\n",
    "\n",
    "    def _get_lidar_path(self, agent_input: AgentInput) -> str:\n",
    "        \"\"\"\n",
    "        Extract lidar path from AgentInput\n",
    "        :param agent_input: input dataclass\n",
    "        :return: lidar path as string\n",
    "        \"\"\"\n",
    "        lidar = agent_input.lidars[-1]\n",
    "\n",
    "        return str(lidar.lidar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c5dfa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from enum import IntEnum\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import cv2\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "from nuplan.common.actor_state.oriented_box import OrientedBox\n",
    "from nuplan.common.actor_state.state_representation import StateSE2\n",
    "from nuplan.common.actor_state.tracked_objects_types import TrackedObjectType\n",
    "from nuplan.common.maps.abstract_map import AbstractMap, MapObject, SemanticMapLayer\n",
    "from nuplan.planning.simulation.trajectory.trajectory_sampling import TrajectorySampling\n",
    "from shapely import affinity\n",
    "from shapely.geometry import LineString, Polygon\n",
    "from torchvision import transforms\n",
    "from navsim.agents.vad.vad_config import VADConfig\n",
    "from navsim.common.dataclasses import AgentInput, Annotations, Scene\n",
    "from navsim.common.enums import BoundingBoxIndex, LidarIndex\n",
    "from navsim.planning.scenario_builder.navsim_scenario_utils import tracked_object_types\n",
    "from navsim.planning.training.abstract_feature_target_builder import AbstractFeatureBuilder, AbstractTargetBuilder\n",
    "\n",
    "class VADFeatureBuilder(AbstractFeatureBuilder):\n",
    "    \"\"\"Input feature builder for VAD.\"\"\"\n",
    "\n",
    "    def __init__(self, config: VADConfig):\n",
    "        \"\"\"\n",
    "        Initializes feature builder.\n",
    "        :param config: global config dataclass of VAD\n",
    "        \"\"\"\n",
    "        self._config = config\n",
    "        self.img2tensor = transforms.ToTensor()\n",
    "\n",
    "        self.camera_names = ['cam_f0', 'cam_l0', 'cam_l1', 'cam_l2', 'cam_r0', 'cam_r1', 'cam_r2', 'cam_b0']\n",
    "\n",
    "    def get_unique_name(self) -> str:\n",
    "        \"\"\"Inherited, see superclass.\"\"\"\n",
    "        return \"vad_feature\"\n",
    "\n",
    "    def compute_features(self, agent_input: AgentInput) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Inherited, see superclass.\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Aligned features matching NuScenes format\n",
    "        features.update(self._align_with_nuscenes_format(agent_input))\n",
    "\n",
    "        # Sensor data aligned with NuScenes structure\n",
    "        features.update(self._get_sensor_paths_aligned(agent_input))\n",
    "\n",
    "        # Temporal sensor data (NavSim extension)\n",
    "        features.update(self._get_sensor_data(agent_input))\n",
    "\n",
    "        # Camera calibration\n",
    "        features.update(self._get_camera_calibration(agent_input))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _get_sensor_data(self, agent_input: AgentInput) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract sensor data with temporal hierarchy.\n",
    "        :param agent_input: input dataclass\n",
    "        :return: hierarchical sensor data\n",
    "        \"\"\"\n",
    "        num_frames = len(agent_input.cameras)\n",
    "\n",
    "        # Image paths with temporal structure\n",
    "        image_paths_temporal = {}\n",
    "        lidar_paths_temporal = []\n",
    "\n",
    "        for frame_idx in range(num_frames):\n",
    "            cameras = agent_input.cameras[frame_idx]\n",
    "            lidar = agent_input.lidars[frame_idx]\n",
    "\n",
    "            # Camera paths for this frame\n",
    "            frame_paths = {}\n",
    "            for cam_name in self.camera_names:\n",
    "                cam_data = getattr(cameras, cam_name)\n",
    "                if cam_data.camera_path is not None:\n",
    "                    frame_paths[cam_name] = str(cam_data.camera_path)\n",
    "                else:\n",
    "                    frame_paths[cam_name] = \"\"\n",
    "\n",
    "            image_paths_temporal[f\"frame_{frame_idx}\"] = frame_paths\n",
    "\n",
    "            # LiDAR path for this frame\n",
    "            lidar_path = str(lidar.lidar_path) if lidar.lidar_path is not None else \"\"\n",
    "            lidar_paths_temporal.append(lidar_path)\n",
    "\n",
    "        # Also provide current frame for backward compatibility\n",
    "        current_image_paths = image_paths_temporal[f\"frame_{num_frames-1}\"]\n",
    "        current_lidar_path = lidar_paths_temporal[-1]\n",
    "\n",
    "        return {\n",
    "            \"image_paths_temporal\": image_paths_temporal,  # Dict[str, Dict[str, str]] - frame_idx -> cam_name -> path\n",
    "            \"lidar_paths_temporal\": lidar_paths_temporal,  # List[str] - temporal lidar paths\n",
    "            \"image_paths\": current_image_paths,  # Dict[str, str] - current frame only (backward compatibility)\n",
    "            \"lidar_path\": current_lidar_path,    # str - current frame only (backward compatibility)\n",
    "        }\n",
    "\n",
    "    def _get_ego_status(self, agent_input: AgentInput) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract comprehensive ego status from AgentInput.\n",
    "        :param agent_input: input dataclass\n",
    "        :return: ego status as dict of torch tensors\n",
    "        \"\"\"\n",
    "        current_ego = agent_input.ego_statuses[-1]  # Current frame\n",
    "\n",
    "        # Basic ego state\n",
    "        ego_features = {\n",
    "            \"ego_pose\": torch.tensor(current_ego.ego_pose, dtype=torch.float32),  # [x, y, heading]\n",
    "            \"ego_velocity\": torch.tensor(current_ego.ego_velocity, dtype=torch.float32),  # [vx, vy]\n",
    "            \"ego_acceleration\": torch.tensor(current_ego.ego_acceleration, dtype=torch.float32),  # [ax, ay]\n",
    "            \"driving_command\": torch.tensor(current_ego.driving_command, dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        # Extended ego features (matching original NuScenes format)\n",
    "        velocity_magnitude = np.linalg.norm(current_ego.ego_velocity)\n",
    "\n",
    "        # Convert driving command to one-hot format for ego_fut_cmd\n",
    "        driving_cmd_onehot = np.zeros(3, dtype=np.float32)\n",
    "        if len(current_ego.driving_command) > 0:\n",
    "            cmd = current_ego.driving_command[0] if hasattr(current_ego.driving_command, '__len__') else current_ego.driving_command\n",
    "            if cmd == 0:  # Right\n",
    "                driving_cmd_onehot[0] = 1.0\n",
    "            elif cmd == 1:  # Left\n",
    "                driving_cmd_onehot[1] = 1.0\n",
    "            else:  # Straight\n",
    "                driving_cmd_onehot[2] = 1.0\n",
    "        else:\n",
    "            driving_cmd_onehot[2] = 1.0  # Default to straight\n",
    "\n",
    "        ego_features.update({\n",
    "            \"ego_speed\": torch.tensor(velocity_magnitude, dtype=torch.float32),\n",
    "            \"ego_fut_cmd\": torch.tensor(driving_cmd_onehot, dtype=torch.float32),  # [Turn Right, Turn Left, Go Straight]\n",
    "            \"ego_heading\": torch.tensor(current_ego.ego_pose[2], dtype=torch.float32),  # heading from pose\n",
    "        })\n",
    "\n",
    "        return ego_features\n",
    "\n",
    "    def _get_ego_trajectories(self, agent_input: AgentInput) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract ego historical trajectory from AgentInput.\n",
    "        :param agent_input: input dataclass\n",
    "        :return: ego trajectory information\n",
    "        \"\"\"\n",
    "        num_history = len(agent_input.ego_statuses)\n",
    "\n",
    "        # Historical poses (already in local coordinates)\n",
    "        ego_poses = np.array([status.ego_pose for status in agent_input.ego_statuses], dtype=np.float32)\n",
    "\n",
    "        # Calculate historical trajectory as offsets\n",
    "        if num_history > 1:\n",
    "            ego_his_trajs = ego_poses[1:, :2] - ego_poses[:-1, :2]  # Offset format\n",
    "        else:\n",
    "            ego_his_trajs = np.zeros((0, 2), dtype=np.float32)\n",
    "\n",
    "        return {\n",
    "            \"ego_his_trajs\": torch.tensor(ego_his_trajs, dtype=torch.float32),  # [his_ts, 2]\n",
    "            \"ego_poses_history\": torch.tensor(ego_poses[:, :2], dtype=torch.float32),  # [his_ts+1, 2] absolute positions\n",
    "        }\n",
    "\n",
    "    def _get_camera_calibration(self, agent_input: AgentInput) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract camera calibration information from AgentInput.\n",
    "        :param agent_input: input dataclass\n",
    "        :return: camera calibration as dict of torch tensors\n",
    "        \"\"\"\n",
    "        cameras = agent_input.cameras[-1]  # Current frame cameras\n",
    "\n",
    "        intrinsics = {}\n",
    "        cam2lidar_extrinsics = {}\n",
    "        lidar2cam_extrinsics = {}\n",
    "        img2lidar_transforms = {}\n",
    "        lidar2img_transforms = {}\n",
    "\n",
    "        camera_names = ['cam_f0', 'cam_l0', 'cam_l1', 'cam_l2', 'cam_r0', 'cam_r1', 'cam_r2', 'cam_b0']\n",
    "\n",
    "        for cam_name in camera_names:\n",
    "            cam_data = getattr(cameras, cam_name)\n",
    "\n",
    "            # Intrinsic parameters\n",
    "            if cam_data.intrinsics is not None:\n",
    "                intrinsics[cam_name] = torch.tensor(cam_data.intrinsics, dtype=torch.float32)\n",
    "\n",
    "            # Extrinsic transformations\n",
    "            if (cam_data.sensor2lidar_rotation is not None and\n",
    "                cam_data.sensor2lidar_translation is not None and\n",
    "                cam_data.intrinsics is not None):\n",
    "\n",
    "                # Create 4x4 transformation matrix from camera to lidar\n",
    "                cam2lidar_transform = np.eye(4, dtype=np.float32)\n",
    "                cam2lidar_transform[:3, :3] = cam_data.sensor2lidar_rotation\n",
    "                cam2lidar_transform[:3, 3] = cam_data.sensor2lidar_translation\n",
    "                cam2lidar_extrinsics[cam_name] = torch.tensor(cam2lidar_transform, dtype=torch.float32)\n",
    "\n",
    "                # Inverse transformation: lidar to camera\n",
    "                lidar2cam_transform = np.linalg.inv(cam2lidar_transform)\n",
    "                lidar2cam_extrinsics[cam_name] = torch.tensor(lidar2cam_transform, dtype=torch.float32)\n",
    "\n",
    "                # Image to lidar: K^-1 @ cam2lidar\n",
    "                K_inv = np.linalg.inv(cam_data.intrinsics)\n",
    "                K_inv_4x4 = np.eye(4, dtype=np.float32)\n",
    "                K_inv_4x4[:3, :3] = K_inv\n",
    "                img2lidar_transform = cam2lidar_transform @ K_inv_4x4\n",
    "                img2lidar_transforms[cam_name] = torch.tensor(img2lidar_transform, dtype=torch.float32)\n",
    "\n",
    "                # Lidar to image: lidar2cam @ K\n",
    "                K_4x4 = np.eye(4, dtype=np.float32)\n",
    "                K_4x4[:3, :3] = cam_data.intrinsics\n",
    "                lidar2img_transform = K_4x4 @ lidar2cam_transform\n",
    "                lidar2img_transforms[cam_name] = torch.tensor(lidar2img_transform, dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"intrinsics\": intrinsics,  # Dict[str, torch.Tensor] - cam_name -> 3x3 intrinsic matrix\n",
    "            \"cam2lidar\": cam2lidar_extrinsics,  # Dict[str, torch.Tensor] - cam_name -> 4x4 transform matrix\n",
    "            \"lidar2cam\": lidar2cam_extrinsics,  # Dict[str, torch.Tensor] - cam_name -> 4x4 transform matrix\n",
    "            \"img2lidar\": img2lidar_transforms,  # Dict[str, torch.Tensor] - cam_name -> 4x4 transform matrix\n",
    "            \"lidar2img\": lidar2img_transforms,  # Dict[str, torch.Tensor] - cam_name -> 4x4 transform matrix\n",
    "        }\n",
    "\n",
    "    def _get_temporal_info(self, agent_input: AgentInput) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract temporal information from AgentInput.\n",
    "        :param agent_input: input dataclass\n",
    "        :return: temporal information\n",
    "        \"\"\"\n",
    "        num_history = len(agent_input.ego_statuses)\n",
    "\n",
    "        return {\n",
    "            \"num_history_frames\": torch.tensor(num_history, dtype=torch.int64),\n",
    "            \"frame_sequence_length\": torch.tensor(num_history, dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "    def _get_images(self, agent_input: AgentInput) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract images from AgentInput.\n",
    "        :param agent_input: input dataclass\n",
    "        :return: images as torch tensors\n",
    "        \"\"\"\n",
    "        cameras = agent_input.cameras[-1]\n",
    "        images = {}\n",
    "\n",
    "        camera_names = ['cam_f0', 'cam_l0', 'cam_l1', 'cam_l2', 'cam_r0', 'cam_r1', 'cam_r2', 'cam_b0']\n",
    "\n",
    "        for cam_name in camera_names:\n",
    "            cam_data = getattr(cameras, cam_name)\n",
    "            if cam_data.image is not None:\n",
    "                images[cam_name] = self.img2tensor(cam_data.image)\n",
    "\n",
    "        return images\n",
    "\n",
    "    def _get_image_paths(self, agent_input: AgentInput) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Extract image paths from AgentInput.\n",
    "        :param agent_input: input dataclass\n",
    "        :return: image paths as strings\n",
    "        \"\"\"\n",
    "        cameras = agent_input.cameras[-1]\n",
    "        paths = {}\n",
    "\n",
    "        camera_names = ['cam_f0', 'cam_l0', 'cam_l1', 'cam_l2', 'cam_r0', 'cam_r1', 'cam_r2', 'cam_b0']\n",
    "\n",
    "        for cam_name in camera_names:\n",
    "            cam_data = getattr(cameras, cam_name)\n",
    "            if cam_data.camera_path is not None:\n",
    "                paths[cam_name] = str(cam_data.camera_path)\n",
    "\n",
    "        return paths\n",
    "\n",
    "    def _get_lidar_path(self, agent_input: AgentInput) -> str:\n",
    "        \"\"\"\n",
    "        Extract lidar path from AgentInput.\n",
    "        :param agent_input: input dataclass\n",
    "        :return: lidar path as string\n",
    "        \"\"\"\n",
    "        lidar = agent_input.lidars[-1]\n",
    "        return str(lidar.lidar_path) if lidar.lidar_path is not None else \"\"\n",
    "\n",
    "    def _get_lidar_sweeps(self, agent_input: AgentInput) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract LiDAR sweep information for temporal context.\n",
    "        :param agent_input: input dataclass\n",
    "        :return: list of sweep information\n",
    "        \"\"\"\n",
    "        sweeps = []\n",
    "\n",
    "        for i, lidar in enumerate(agent_input.lidars[:-1]):  # Historical LiDAR data\n",
    "            if lidar.lidar_path is not None:\n",
    "                sweep_info = {\n",
    "                    \"data_path\": str(lidar.lidar_path),\n",
    "                    \"frame_idx\": i,\n",
    "                    \"is_current\": False,\n",
    "                }\n",
    "                sweeps.append(sweep_info)\n",
    "\n",
    "        # Add current frame\n",
    "        current_lidar = agent_input.lidars[-1]\n",
    "        if current_lidar.lidar_path is not None:\n",
    "            sweeps.append({\n",
    "                \"data_path\": str(current_lidar.lidar_path),\n",
    "                \"frame_idx\": len(agent_input.lidars) - 1,\n",
    "                \"is_current\": True,\n",
    "            })\n",
    "\n",
    "        return sweeps\n",
    "\n",
    "\n",
    "    def _get_sensor_paths_aligned(self, agent_input: AgentInput) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get sensor paths in format aligned with original NuScenes structure.\n",
    "        :param agent_input: input dataclass\n",
    "        :return: sensor paths in NuScenes-like format\n",
    "        \"\"\"\n",
    "        current_cameras = agent_input.cameras[-1]\n",
    "        current_lidar = agent_input.lidars[-1]\n",
    "\n",
    "        # Camera paths (using all 8 cameras with original cam_f0 naming)\n",
    "        cams = {}\n",
    "\n",
    "        for cam_name in self.camera_names:  # ['cam_f0', 'cam_l0', 'cam_l1', 'cam_l2', 'cam_r0', 'cam_r1', 'cam_r2', 'cam_b0']\n",
    "            cam_data = getattr(current_cameras, cam_name)\n",
    "            if cam_data.camera_path is not None:\n",
    "                cams[cam_name] = {\n",
    "                    'data_path': str(cam_data.camera_path),\n",
    "                    'cam_intrinsic': cam_data.intrinsics.tolist() if cam_data.intrinsics is not None else None,\n",
    "                    'sensor2lidar_rotation': cam_data.sensor2lidar_rotation.tolist() if cam_data.sensor2lidar_rotation is not None else None,\n",
    "                    'sensor2lidar_translation': cam_data.sensor2lidar_translation.tolist() if cam_data.sensor2lidar_translation is not None else None,\n",
    "                }\n",
    "            else:\n",
    "                # Empty camera entry if no data available\n",
    "                cams[cam_name] = {\n",
    "                    'data_path': \"\",\n",
    "                    'cam_intrinsic': None,\n",
    "                    'sensor2lidar_rotation': None,\n",
    "                    'sensor2lidar_translation': None,\n",
    "                }\n",
    "\n",
    "        return {\n",
    "            \"cams\": cams,  # Dict with cam_f0, cam_l0, cam_l1, cam_l2, cam_r0, cam_r1, cam_r2, cam_b0\n",
    "            \"lidar_path\": str(current_lidar.lidar_path) if current_lidar.lidar_path is not None else \"\",\n",
    "        }\n",
    "\n",
    "    def _get_temporal_sensor_paths(self, agent_input: AgentInput) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get temporal sensor paths for all frames.\n",
    "        :param agent_input: input dataclass\n",
    "        :return: temporal sensor paths\n",
    "        \"\"\"\n",
    "        num_frames = len(agent_input.cameras)\n",
    "\n",
    "        # Temporal camera paths\n",
    "        cams_temporal = {}\n",
    "        lidar_paths_temporal = []\n",
    "\n",
    "        for frame_idx in range(num_frames):\n",
    "            cameras = agent_input.cameras[frame_idx]\n",
    "            lidar = agent_input.lidars[frame_idx]\n",
    "\n",
    "            # Camera paths for this frame\n",
    "            frame_cams = {}\n",
    "            for cam_name in self.camera_names:\n",
    "                cam_data = getattr(cameras, cam_name)\n",
    "                frame_cams[cam_name] = {\n",
    "                    'data_path': str(cam_data.camera_path) if cam_data.camera_path is not None else \"\",\n",
    "                    'cam_intrinsic': cam_data.intrinsics.tolist() if cam_data.intrinsics is not None else None,\n",
    "                    'sensor2lidar_rotation': cam_data.sensor2lidar_rotation.tolist() if cam_data.sensor2lidar_rotation is not None else None,\n",
    "                    'sensor2lidar_translation': cam_data.sensor2lidar_translation.tolist() if cam_data.sensor2lidar_translation is not None else None,\n",
    "                }\n",
    "\n",
    "            cams_temporal[f\"frame_{frame_idx}\"] = frame_cams\n",
    "\n",
    "            # LiDAR path for this frame\n",
    "            lidar_path = str(lidar.lidar_path) if lidar.lidar_path is not None else \"\"\n",
    "            lidar_paths_temporal.append(lidar_path)\n",
    "\n",
    "        return {\n",
    "            \"cams_temporal\": cams_temporal,  # Dict[str, Dict[str, Dict]] - frame_idx -> cam_name -> cam_info\n",
    "            \"lidar_paths_temporal\": lidar_paths_temporal,  # List[str] - temporal lidar paths\n",
    "        }\n",
    "\n",
    "    def _align_with_nuscenes_format(self, agent_input: AgentInput) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Align with original NuScenes data format keys and structure.\n",
    "        :param agent_input: input dataclass\n",
    "        :return: aligned features matching NuScenes format\n",
    "        \"\"\"\n",
    "        current_ego = agent_input.ego_statuses[-1]\n",
    "        num_history = len(agent_input.ego_statuses)\n",
    "\n",
    "        # Basic aligned keys\n",
    "        aligned_features = {\n",
    "            # Temporal info (matching NuScenes format)\n",
    "            \"frame_idx\": torch.tensor(num_history - 1, dtype=torch.int64),  # Current frame index\n",
    "            \"frame_sequence_length\": torch.tensor(num_history, dtype=torch.int64),\n",
    "            \"timestamp\": torch.tensor(0, dtype=torch.int64),  # NavSim doesn't have direct timestamp access\n",
    "\n",
    "            # Ego pose (matching NuScenes: [x, y, heading])\n",
    "            \"ego_pose\": torch.tensor(current_ego.ego_pose, dtype=torch.float32),  # Local coordinates\n",
    "            \"ego_velocity\": torch.tensor(current_ego.ego_velocity, dtype=torch.float32),  # [vx, vy]\n",
    "            \"ego_acceleration\": torch.tensor(current_ego.ego_acceleration, dtype=torch.float32),  # [ax, ay]\n",
    "\n",
    "            # Extended ego features (matching original ego_lcf_feat format)\n",
    "            \"ego_speed\": torch.tensor(np.linalg.norm(current_ego.ego_velocity), dtype=torch.float32),\n",
    "            \"ego_heading\": torch.tensor(current_ego.ego_pose[2], dtype=torch.float32),\n",
    "\n",
    "            # Vehicle dimensions (using constants from original code)\n",
    "            \"ego_length\": torch.tensor(4.084, dtype=torch.float32),  # From original: ego_length\n",
    "            \"ego_width\": torch.tensor(1.85, dtype=torch.float32),   # From original: ego_width\n",
    "\n",
    "            # Driving command (convert to one-hot like original ego_fut_cmd)\n",
    "            \"driving_command\": torch.tensor(current_ego.driving_command, dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        # Convert driving command to one-hot format (matching original format)\n",
    "        driving_cmd_onehot = np.zeros(3, dtype=np.float32)\n",
    "        if hasattr(current_ego.driving_command, '__len__') and len(current_ego.driving_command) > 0:\n",
    "            cmd = current_ego.driving_command[0]\n",
    "        else:\n",
    "            cmd = current_ego.driving_command\n",
    "\n",
    "        if cmd == 0:  # Right\n",
    "            driving_cmd_onehot[0] = 1.0\n",
    "        elif cmd == 1:  # Left\n",
    "            driving_cmd_onehot[1] = 1.0\n",
    "        else:  # Straight\n",
    "            driving_cmd_onehot[2] = 1.0\n",
    "\n",
    "        aligned_features[\"ego_fut_cmd\"] = torch.tensor(driving_cmd_onehot, dtype=torch.float32)\n",
    "\n",
    "        # Historical trajectory (matching original ego_his_trajs format)\n",
    "        ego_poses = np.array([status.ego_pose for status in agent_input.ego_statuses], dtype=np.float32)\n",
    "        if num_history > 1:\n",
    "            ego_his_trajs = ego_poses[1:, :2] - ego_poses[:-1, :2]  # Offset format like original\n",
    "        else:\n",
    "            ego_his_trajs = np.zeros((0, 2), dtype=np.float32)\n",
    "\n",
    "        aligned_features[\"ego_his_trajs\"] = torch.tensor(ego_his_trajs, dtype=torch.float32)\n",
    "\n",
    "        # Ego LCF feat (matching original 9-element format: vx, vy, ax, ay, w, length, width, vel, steer)\n",
    "        ego_lcf_feat = np.zeros(9, dtype=np.float32)\n",
    "        ego_lcf_feat[0:2] = current_ego.ego_velocity  # vx, vy\n",
    "        ego_lcf_feat[2:4] = current_ego.ego_acceleration  # ax, ay\n",
    "        ego_lcf_feat[4] = 0.0  # yaw rate (w) - not available in AgentInput\n",
    "        ego_lcf_feat[5] = 4.084  # ego_length\n",
    "        ego_lcf_feat[6] = 1.85   # ego_width\n",
    "        ego_lcf_feat[7] = np.linalg.norm(current_ego.ego_velocity)  # vel magnitude\n",
    "        ego_lcf_feat[8] = 0.0  # steering (Kappa) - not available in AgentInput\n",
    "\n",
    "        aligned_features[\"ego_lcf_feat\"] = torch.tensor(ego_lcf_feat, dtype=torch.float32)\n",
    "\n",
    "        return aligned_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21d4e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundingBox2DIndex(IntEnum):\n",
    "    \"\"\"Intenum for bounding boxes in TransFuser.\"\"\"\n",
    "\n",
    "    _X = 0\n",
    "    _Y = 1\n",
    "    _HEADING = 2\n",
    "    _LENGTH = 3\n",
    "    _WIDTH = 4\n",
    "\n",
    "    @classmethod\n",
    "    def size(cls):\n",
    "        valid_attributes = [\n",
    "            attribute\n",
    "            for attribute in dir(cls)\n",
    "            if attribute.startswith(\"_\") and not attribute.startswith(\"__\") and not callable(getattr(cls, attribute))\n",
    "        ]\n",
    "        return len(valid_attributes)\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    def X(cls):\n",
    "        return cls._X\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    def Y(cls):\n",
    "        return cls._Y\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    def HEADING(cls):\n",
    "        return cls._HEADING\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    def LENGTH(cls):\n",
    "        return cls._LENGTH\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    def WIDTH(cls):\n",
    "        return cls._WIDTH\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    def POINT(cls):\n",
    "        # assumes X, Y have subsequent indices\n",
    "        return slice(cls._X, cls._Y + 1)\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    def STATE_SE2(cls):\n",
    "        # assumes X, Y, HEADING have subsequent indices\n",
    "        return slice(cls._X, cls._HEADING + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b2be1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VADTargetBuilder(AbstractTargetBuilder):\n",
    "    \"\"\"Output target builder for VAD with full NuScenes compatibility.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        trajectory_sampling: TrajectorySampling,\n",
    "        config: VADConfig,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes target builder.\n",
    "        :param trajectory_sampling: trajectory sampling specification\n",
    "        :param config: global config dataclass of VAD\n",
    "        \"\"\"\n",
    "        self._trajectory_sampling = trajectory_sampling\n",
    "        self._config = config\n",
    "\n",
    "        # Agent type mapping for NavSim to NuScenes categories\n",
    "        self.agent_type_mapping = {\n",
    "            \"vehicle\": 0,\n",
    "            \"pedestrian\": 7,\n",
    "            \"bicycle\": 5,\n",
    "            \"motorcycle\": 6,\n",
    "            \"bus\": 3,\n",
    "            \"trailer\": 2,\n",
    "            \"construction_vehicle\": 4,\n",
    "            \"traffic_cone\": 8,\n",
    "            \"barrier\": 9,\n",
    "        }\n",
    "\n",
    "        # NuScenes category names\n",
    "        self.nus_categories = ('car', 'truck', 'trailer', 'bus', 'construction_vehicle',\n",
    "                              'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier')\n",
    "\n",
    "    def get_unique_name(self) -> str:\n",
    "        \"\"\"Inherited, see superclass.\"\"\"\n",
    "        return \"vad_target\"\n",
    "\n",
    "    def compute_targets(self, scene: Scene) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inherited, see superclass.\n",
    "        :param scene: Scene dataclass containing all ground truth information\n",
    "        :return: Dictionary containing all target information for VAD training\n",
    "        \"\"\"\n",
    "        targets = {}\n",
    "\n",
    "        # Basic trajectory and agent targets (from original VAD)\n",
    "        trajectory = torch.tensor(\n",
    "            scene.get_future_trajectory(num_trajectory_frames=self._trajectory_sampling.num_poses).poses\n",
    "        )\n",
    "\n",
    "        current_frame_idx = scene.scene_metadata.num_history_frames - 1\n",
    "        current_frame = scene.frames[current_frame_idx]\n",
    "        annotations = current_frame.annotations\n",
    "        ego_pose = StateSE2(*current_frame.ego_status.ego_pose)\n",
    "\n",
    "        agent_states, agent_labels = self._compute_agent_targets(annotations)\n",
    "        bev_semantic_map = self._compute_bev_semantic_map(annotations, scene.map_api, ego_pose)\n",
    "\n",
    "        # Basic targets from original VAD\n",
    "        targets.update({\n",
    "            \"trajectory\": trajectory,\n",
    "            \"agent_states\": agent_states,\n",
    "            \"agent_labels\": agent_labels,\n",
    "            \"bev_semantic_map\": bev_semantic_map,\n",
    "        })\n",
    "\n",
    "        # Additional GT info that was missing in AgentInput\n",
    "        targets.update(self._get_scene_gt_info(scene))\n",
    "        targets.update(self._get_agent_future_trajectories(scene))\n",
    "        targets.update(self._get_agent_lcf_features(scene))\n",
    "        targets.update(self._get_nuscenes_compatibility_info(scene))\n",
    "\n",
    "        return targets\n",
    "\n",
    "    def _compute_agent_targets(self, annotations: Annotations) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extracts 2D agent bounding boxes in ego coordinates.\n",
    "        :param annotations: annotation dataclass\n",
    "        :return: tuple of bounding box values and labels (binary)\n",
    "        \"\"\"\n",
    "        # Use default value if num_bounding_boxes is not in config\n",
    "        max_agents = getattr(self._config, 'num_bounding_boxes', 200)  # Default to 200\n",
    "        agent_states_list: List[npt.NDArray[np.float32]] = []\n",
    "\n",
    "        def _xy_in_lidar(x: float, y: float) -> bool:\n",
    "            # Use default lidar range if not in config\n",
    "            lidar_min_x = getattr(self._config, 'lidar_min_x', -50.0)\n",
    "            lidar_max_x = getattr(self._config, 'lidar_max_x', 50.0)\n",
    "            lidar_min_y = getattr(self._config, 'lidar_min_y', -50.0)\n",
    "            lidar_max_y = getattr(self._config, 'lidar_max_y', 50.0)\n",
    "            return (lidar_min_x <= x <= lidar_max_x) and (lidar_min_y <= y <= lidar_max_y)\n",
    "\n",
    "        if annotations is not None:\n",
    "            for box, name in zip(annotations.boxes, annotations.names):\n",
    "                # NavSim box format: [x, y, z, length, width, height, heading]\n",
    "                # Use direct indices instead of enum\n",
    "                box_x, box_y, box_heading, box_length, box_width = (\n",
    "                    box[0],  # X\n",
    "                    box[1],  # Y\n",
    "                    box[6],  # HEADING (last element)\n",
    "                    box[3],  # LENGTH\n",
    "                    box[4],  # WIDTH\n",
    "                )\n",
    "\n",
    "                if name == \"vehicle\" and _xy_in_lidar(box_x, box_y):\n",
    "                    agent_states_list.append(\n",
    "                        np.array(\n",
    "                            [box_x, box_y, box_heading, box_length, box_width],\n",
    "                            dtype=np.float32,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        agents_states_arr = np.array(agent_states_list)\n",
    "\n",
    "        # Filter num_instances nearest\n",
    "        agent_states = np.zeros((max_agents, 5), dtype=np.float32)  # Use fixed size 5\n",
    "        agent_labels = np.zeros(max_agents, dtype=bool)\n",
    "\n",
    "        if len(agents_states_arr) > 0:\n",
    "            # Calculate distances using x, y coordinates (first 2 elements)\n",
    "            distances = np.linalg.norm(agents_states_arr[:, :2], axis=-1)\n",
    "            argsort = np.argsort(distances)[:max_agents]\n",
    "\n",
    "            # Filter detections\n",
    "            agents_states_arr = agents_states_arr[argsort]\n",
    "            agent_states[: len(agents_states_arr)] = agents_states_arr\n",
    "            agent_labels[: len(agents_states_arr)] = True\n",
    "\n",
    "        return torch.tensor(agent_states), torch.tensor(agent_labels)\n",
    "\n",
    "    def _compute_bev_semantic_map(\n",
    "        self, annotations: Annotations, map_api: AbstractMap, ego_pose: StateSE2\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Creates semantic map in BEV.\n",
    "        :param annotations: annotation dataclass\n",
    "        :param map_api: map interface of nuPlan\n",
    "        :param ego_pose: ego pose in global frame\n",
    "        :return: 2D torch tensor of semantic labels\n",
    "        \"\"\"\n",
    "        bev_semantic_map = np.zeros(self._config.bev_semantic_frame, dtype=np.int64)\n",
    "\n",
    "        if map_api is not None:\n",
    "            for label, (entity_type, layers) in self._config.bev_semantic_classes.items():\n",
    "                if entity_type == \"polygon\":\n",
    "                    entity_mask = self._compute_map_polygon_mask(map_api, ego_pose, layers)\n",
    "                elif entity_type == \"linestring\":\n",
    "                    entity_mask = self._compute_map_linestring_mask(map_api, ego_pose, layers)\n",
    "                else:\n",
    "                    entity_mask = self._compute_box_mask(annotations, layers)\n",
    "                bev_semantic_map[entity_mask] = label\n",
    "\n",
    "        return torch.tensor(bev_semantic_map, dtype=torch.int64)\n",
    "\n",
    "    def _get_scene_gt_info(self, scene: Scene) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract ground truth information from Scene that was missing in AgentInput.\n",
    "        :param scene: Scene dataclass\n",
    "        :return: GT information as dict of torch tensors\n",
    "        \"\"\"\n",
    "        current_frame_idx = scene.scene_metadata.num_history_frames - 1\n",
    "        current_frame = scene.frames[current_frame_idx]\n",
    "        annotations = current_frame.annotations\n",
    "\n",
    "        gt_info = {}\n",
    "\n",
    "        # Scene metadata (previously unavailable in AgentInput)\n",
    "        gt_info.update({\n",
    "            \"scene_token\": scene.scene_metadata.scene_token,\n",
    "            \"log_name\": scene.scene_metadata.log_name,\n",
    "            \"map_location\": scene.scene_metadata.map_name,\n",
    "            \"timestamp\": torch.tensor(current_frame.timestamp, dtype=torch.int64),\n",
    "            \"roadblock_ids\": current_frame.roadblock_ids,  # List[str]\n",
    "            \"traffic_lights\": current_frame.traffic_lights,  # List[Tuple[str, bool]]\n",
    "        })\n",
    "\n",
    "        # Agent annotations (previously impossible with AgentInput)\n",
    "        if annotations is not None and len(annotations.boxes) > 0:\n",
    "            # Convert NavSim box format to NuScenes format\n",
    "            # NavSim: [x, y, z, length, width, height, heading]\n",
    "            # NuScenes: [x, y, z, width, length, height, yaw] with rotation adjustment\n",
    "            gt_boxes_nuscenes = np.zeros((len(annotations.boxes), 7), dtype=np.float32)\n",
    "            gt_names_nuscenes = []\n",
    "\n",
    "            for i, (box, name) in enumerate(zip(annotations.boxes, annotations.names)):\n",
    "                # Position (x, y, z) - indices 0, 1, 2\n",
    "                gt_boxes_nuscenes[i, 0:3] = box[0:3]\n",
    "                # Dimensions (convert length, width, height -> width, length, height)\n",
    "                gt_boxes_nuscenes[i, 3] = box[4]  # width (index 4)\n",
    "                gt_boxes_nuscenes[i, 4] = box[3]  # length (index 3)\n",
    "                gt_boxes_nuscenes[i, 5] = box[5]  # height (index 5)\n",
    "                # Rotation (convert heading to NuScenes yaw format)\n",
    "                gt_boxes_nuscenes[i, 6] = -box[6] - np.pi / 2  # heading (index 6)\n",
    "\n",
    "                # Map NavSim names to NuScenes categories\n",
    "                if name in self.agent_type_mapping:\n",
    "                    category_idx = self.agent_type_mapping[name]\n",
    "                    if category_idx < len(self.nus_categories):\n",
    "                        gt_names_nuscenes.append(self.nus_categories[category_idx])\n",
    "                    else:\n",
    "                        gt_names_nuscenes.append(name)\n",
    "                else:\n",
    "                    gt_names_nuscenes.append(name)\n",
    "\n",
    "            gt_info.update({\n",
    "                \"gt_boxes\": torch.tensor(gt_boxes_nuscenes, dtype=torch.float32),  # [N, 7] NuScenes format\n",
    "                \"gt_names\": gt_names_nuscenes,  # List[str]\n",
    "                \"gt_velocity_3d\": torch.tensor(annotations.velocity_3d, dtype=torch.float32),  # [N, 3]\n",
    "                \"gt_velocity\": torch.tensor(annotations.velocity_3d[:, :2], dtype=torch.float32),  # [N, 2] for compatibility\n",
    "                \"instance_tokens\": annotations.instance_tokens,  # List[str]\n",
    "                \"track_tokens\": annotations.track_tokens,  # List[str]\n",
    "                \"num_agents\": torch.tensor(len(annotations.boxes), dtype=torch.int64),\n",
    "                # NuScenes compatibility - NavSim doesn't have lidar/radar point counts\n",
    "                \"num_lidar_pts\": torch.zeros(len(annotations.boxes), dtype=torch.int64),\n",
    "                \"num_radar_pts\": torch.zeros(len(annotations.boxes), dtype=torch.int64),\n",
    "            })\n",
    "\n",
    "            # Valid flag (whether agents actually exist)\n",
    "            valid_flag = np.ones(len(annotations.boxes), dtype=bool)  # All annotations in NavSim are valid\n",
    "            gt_info[\"valid_flag\"] = torch.tensor(valid_flag, dtype=torch.bool)\n",
    "        else:\n",
    "            # Empty annotations\n",
    "            gt_info.update({\n",
    "                \"gt_boxes\": torch.zeros((0, 7), dtype=torch.float32),\n",
    "                \"gt_names\": [],\n",
    "                \"gt_velocity_3d\": torch.zeros((0, 3), dtype=torch.float32),\n",
    "                \"gt_velocity\": torch.zeros((0, 2), dtype=torch.float32),\n",
    "                \"instance_tokens\": [],\n",
    "                \"track_tokens\": [],\n",
    "                \"num_agents\": torch.tensor(0, dtype=torch.int64),\n",
    "                \"valid_flag\": torch.zeros((0,), dtype=torch.bool),\n",
    "                \"num_lidar_pts\": torch.zeros((0,), dtype=torch.int64),\n",
    "                \"num_radar_pts\": torch.zeros((0,), dtype=torch.int64),\n",
    "            })\n",
    "\n",
    "        # Future trajectory (now possible with Scene!)\n",
    "        future_traj = scene.get_future_trajectory(num_trajectory_frames=self._trajectory_sampling.num_poses)\n",
    "        gt_info.update({\n",
    "            \"gt_ego_fut_trajs\": torch.tensor(future_traj.poses[:, :2], dtype=torch.float32),  # [fut_ts, 2]\n",
    "            \"gt_ego_fut_masks\": torch.ones(len(future_traj.poses), dtype=torch.float32),  # All valid\n",
    "            \"fut_valid_flag\": torch.tensor(True, dtype=torch.bool),\n",
    "        })\n",
    "\n",
    "        return gt_info\n",
    "\n",
    "    def _get_agent_future_trajectories(self, scene: Scene) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract agent future trajectories using track_tokens.\n",
    "        :param scene: Scene dataclass\n",
    "        :return: agent future trajectory information\n",
    "        \"\"\"\n",
    "        current_frame_idx = scene.scene_metadata.num_history_frames - 1\n",
    "        current_annotations = scene.frames[current_frame_idx].annotations\n",
    "\n",
    "        if current_annotations is None or len(current_annotations.track_tokens) == 0:\n",
    "            return {\n",
    "                \"gt_agent_fut_trajs\": torch.zeros((0, self._trajectory_sampling.num_poses * 2), dtype=torch.float32),\n",
    "                \"gt_agent_fut_masks\": torch.zeros((0, self._trajectory_sampling.num_poses), dtype=torch.float32),\n",
    "                \"gt_agent_fut_yaw\": torch.zeros((0, self._trajectory_sampling.num_poses), dtype=torch.float32),\n",
    "            }\n",
    "\n",
    "        num_agents = len(current_annotations.track_tokens)\n",
    "        fut_ts = self._trajectory_sampling.num_poses\n",
    "\n",
    "        gt_agent_fut_trajs = np.zeros((num_agents, fut_ts, 2), dtype=np.float32)\n",
    "        gt_agent_fut_masks = np.zeros((num_agents, fut_ts), dtype=np.float32)\n",
    "        gt_agent_fut_yaw = np.zeros((num_agents, fut_ts), dtype=np.float32)\n",
    "\n",
    "        # Track each agent through future frames\n",
    "        for agent_idx, track_token in enumerate(current_annotations.track_tokens):\n",
    "            current_box = current_annotations.boxes[agent_idx]\n",
    "            current_pos = current_box[:2]  # x, y\n",
    "            current_yaw = current_box[6]   # heading\n",
    "\n",
    "            # Look for this agent in future frames\n",
    "            for fut_step in range(fut_ts):\n",
    "                future_frame_idx = current_frame_idx + 1 + fut_step\n",
    "                if future_frame_idx < len(scene.frames):\n",
    "                    future_frame = scene.frames[future_frame_idx]\n",
    "                    if future_frame.annotations is not None:\n",
    "                        # Find agent by track_token in future frame\n",
    "                        if track_token in future_frame.annotations.track_tokens:\n",
    "                            future_agent_idx = future_frame.annotations.track_tokens.index(track_token)\n",
    "                            future_box = future_frame.annotations.boxes[future_agent_idx]\n",
    "                            future_pos = future_box[:2]\n",
    "                            future_yaw = future_box[6]\n",
    "\n",
    "                            # Relative trajectory (offset from current position)\n",
    "                            gt_agent_fut_trajs[agent_idx, fut_step] = future_pos - current_pos\n",
    "                            gt_agent_fut_masks[agent_idx, fut_step] = 1.0\n",
    "                            gt_agent_fut_yaw[agent_idx, fut_step] = future_yaw - current_yaw\n",
    "\n",
    "                            # Update reference for next step\n",
    "                            current_pos = future_pos\n",
    "                            current_yaw = future_yaw\n",
    "                        else:\n",
    "                            # Agent disappeared, stop tracking\n",
    "                            break\n",
    "                    else:\n",
    "                        # No annotations in future frame\n",
    "                        break\n",
    "                else:\n",
    "                    # No more future frames\n",
    "                    break\n",
    "\n",
    "        return {\n",
    "            \"gt_agent_fut_trajs\": torch.tensor(gt_agent_fut_trajs.reshape(num_agents, -1), dtype=torch.float32),  # [N, fut_ts*2]\n",
    "            \"gt_agent_fut_masks\": torch.tensor(gt_agent_fut_masks, dtype=torch.float32),  # [N, fut_ts]\n",
    "            \"gt_agent_fut_yaw\": torch.tensor(gt_agent_fut_yaw, dtype=torch.float32),     # [N, fut_ts]\n",
    "        }\n",
    "\n",
    "    def _get_agent_lcf_features(self, scene: Scene) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract agent LCF (Local Coordinate Frame) features.\n",
    "        :param scene: Scene dataclass\n",
    "        :return: agent features matching original format\n",
    "        \"\"\"\n",
    "        current_frame_idx = scene.scene_metadata.num_history_frames - 1\n",
    "        annotations = scene.frames[current_frame_idx].annotations\n",
    "\n",
    "        if annotations is None or len(annotations.boxes) == 0:\n",
    "            return {\n",
    "                \"gt_agent_lcf_feat\": torch.zeros((0, 9), dtype=torch.float32),\n",
    "                \"gt_agent_fut_goal\": torch.zeros((0,), dtype=torch.float32),\n",
    "            }\n",
    "\n",
    "        num_agents = len(annotations.boxes)\n",
    "        agent_lcf_feat = np.zeros((num_agents, 9), dtype=np.float32)\n",
    "\n",
    "        for i, (box, name) in enumerate(zip(annotations.boxes, annotations.names)):\n",
    "            # agent lcf feat format: (x, y, yaw, vx, vy, width, length, height, type)\n",
    "            agent_lcf_feat[i, 0:2] = box[:2]        # x, y\n",
    "            agent_lcf_feat[i, 2] = box[6]           # yaw/heading\n",
    "            agent_lcf_feat[i, 3:5] = annotations.velocity_3d[i, :2]  # vx, vy\n",
    "            agent_lcf_feat[i, 5] = box[4]           # width\n",
    "            agent_lcf_feat[i, 6] = box[3]           # length\n",
    "            agent_lcf_feat[i, 7] = box[5]           # height\n",
    "            # Type encoding\n",
    "            agent_lcf_feat[i, 8] = self._get_agent_type_id(name)\n",
    "\n",
    "        # Goal classification (simplified version)\n",
    "        gt_agent_fut_goal = np.zeros(num_agents, dtype=np.float32)\n",
    "        agent_fut_trajs_dict = self._get_agent_future_trajectories(scene)\n",
    "        agent_fut_trajs = agent_fut_trajs_dict[\"gt_agent_fut_trajs\"]\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            if len(agent_fut_trajs) > i:\n",
    "                # Get final position offset\n",
    "                traj = agent_fut_trajs[i].reshape(-1, 2)\n",
    "                if len(traj) > 0:\n",
    "                    # Calculate cumulative trajectory\n",
    "                    cumulative_traj = torch.cumsum(traj, dim=0)\n",
    "                    if len(cumulative_traj) > 0:\n",
    "                        final_offset = cumulative_traj[-1]\n",
    "                        if torch.norm(final_offset) < 1.0:  # static\n",
    "                            gt_agent_fut_goal[i] = 9\n",
    "                        else:\n",
    "                            # Direction classification (0-8 for 8 directions)\n",
    "                            goal_yaw = torch.atan2(final_offset[1], final_offset[0]) + np.pi\n",
    "                            gt_agent_fut_goal[i] = goal_yaw // (np.pi / 4)\n",
    "\n",
    "        return {\n",
    "            \"gt_agent_lcf_feat\": torch.tensor(agent_lcf_feat, dtype=torch.float32),\n",
    "            \"gt_agent_fut_goal\": torch.tensor(gt_agent_fut_goal, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "    def _get_nuscenes_compatibility_info(self, scene: Scene) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get additional info for NuScenes compatibility.\n",
    "        :param scene: Scene dataclass\n",
    "        :return: compatibility information\n",
    "        \"\"\"\n",
    "        current_frame_idx = scene.scene_metadata.num_history_frames - 1\n",
    "        current_frame = scene.frames[current_frame_idx]\n",
    "\n",
    "        # Frame connectivity (prev/next tokens)\n",
    "        prev_token = \"\"\n",
    "        next_token = \"\"\n",
    "        if current_frame_idx > 0:\n",
    "            prev_token = scene.frames[current_frame_idx - 1].token\n",
    "        if current_frame_idx < len(scene.frames) - 1:\n",
    "            next_token = scene.frames[current_frame_idx + 1].token\n",
    "\n",
    "        # Can bus equivalent (18-element array like original)\n",
    "        can_bus = np.zeros(18, dtype=np.float32)\n",
    "        ego_status = current_frame.ego_status\n",
    "        can_bus[0:3] = ego_status.ego_pose[:3]  # position\n",
    "        can_bus[3:7] = [0.0, 0.0, 0.0, 1.0]    # rotation quaternion (simplified)\n",
    "        can_bus[7:9] = ego_status.ego_acceleration  # acceleration\n",
    "        can_bus[13:15] = ego_status.ego_velocity    # velocity\n",
    "        # Rest filled with zeros (vehicle dynamics not available in NavSim)\n",
    "\n",
    "        return {\n",
    "            \"token\": current_frame.token,\n",
    "            \"prev\": prev_token,\n",
    "            \"next\": next_token,\n",
    "            \"can_bus\": torch.tensor(can_bus, dtype=torch.float32),\n",
    "            \"frame_idx\": torch.tensor(current_frame_idx, dtype=torch.int64),\n",
    "            # Coordinate transformations (NavSim uses ego coordinate)\n",
    "            \"lidar2ego_translation\": torch.tensor([0.0, 0.0, 0.0], dtype=torch.float32),\n",
    "            \"lidar2ego_rotation\": torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32),\n",
    "            \"ego2global_translation\": torch.tensor(ego_status.ego_pose[:3], dtype=torch.float32),\n",
    "            \"ego2global_rotation\": torch.tensor([0.0, 0.0, 0.0, 1.0], dtype=torch.float32),\n",
    "            # Sweeps (empty for now, could be populated with historical lidar)\n",
    "            \"sweeps\": [],\n",
    "        }\n",
    "\n",
    "    def _get_agent_type_id(self, agent_name: str) -> int:\n",
    "        \"\"\"Map agent name to type ID.\"\"\"\n",
    "        return self.agent_type_mapping.get(agent_name, -1)\n",
    "\n",
    "    def _compute_map_polygon_mask(\n",
    "        self, map_api: AbstractMap, ego_pose: StateSE2, layers: List[SemanticMapLayer]\n",
    "    ) -> npt.NDArray[np.bool_]:\n",
    "        \"\"\"\n",
    "        Compute binary mask given a map layer class.\n",
    "        :param map_api: map interface of nuPlan\n",
    "        :param ego_pose: ego pose in global frame\n",
    "        :param layers: map layers\n",
    "        :return: binary mask as numpy array\n",
    "        \"\"\"\n",
    "        map_object_dict = map_api.get_proximal_map_objects(\n",
    "            point=ego_pose.point, radius=self._config.bev_radius, layers=layers\n",
    "        )\n",
    "        map_polygon_mask = np.zeros(self._config.bev_semantic_frame[::-1], dtype=np.uint8)\n",
    "        for layer in layers:\n",
    "            for map_object in map_object_dict[layer]:\n",
    "                polygon: Polygon = self._geometry_local_coords(map_object.polygon, ego_pose)\n",
    "                exterior = np.array(polygon.exterior.coords).reshape((-1, 1, 2))\n",
    "                exterior = self._coords_to_pixel(exterior)\n",
    "                cv2.fillPoly(map_polygon_mask, [exterior], color=255)\n",
    "        # OpenCV has origin on top-left corner\n",
    "        map_polygon_mask = np.rot90(map_polygon_mask)[::-1]\n",
    "        return map_polygon_mask > 0\n",
    "\n",
    "    def _compute_map_linestring_mask(\n",
    "        self, map_api: AbstractMap, ego_pose: StateSE2, layers: List[SemanticMapLayer]\n",
    "    ) -> npt.NDArray[np.bool_]:\n",
    "        \"\"\"\n",
    "        Compute binary of linestring given a map layer class.\n",
    "        :param map_api: map interface of nuPlan\n",
    "        :param ego_pose: ego pose in global frame\n",
    "        :param layers: map layers\n",
    "        :return: binary mask as numpy array\n",
    "        \"\"\"\n",
    "        map_object_dict = map_api.get_proximal_map_objects(\n",
    "            point=ego_pose.point, radius=self._config.bev_radius, layers=layers\n",
    "        )\n",
    "        map_linestring_mask = np.zeros(self._config.bev_semantic_frame[::-1], dtype=np.uint8)\n",
    "        for layer in layers:\n",
    "            for map_object in map_object_dict[layer]:\n",
    "                linestring: LineString = self._geometry_local_coords(map_object.baseline_path.linestring, ego_pose)\n",
    "                points = np.array(linestring.coords).reshape((-1, 1, 2))\n",
    "                points = self._coords_to_pixel(points)\n",
    "                cv2.polylines(\n",
    "                    map_linestring_mask,\n",
    "                    [points],\n",
    "                    isClosed=False,\n",
    "                    color=255,\n",
    "                    thickness=2,\n",
    "                )\n",
    "        # OpenCV has origin on top-left corner\n",
    "        map_linestring_mask = np.rot90(map_linestring_mask)[::-1]\n",
    "        return map_linestring_mask > 0\n",
    "\n",
    "    def _compute_box_mask(self, annotations: Annotations, layers: TrackedObjectType) -> npt.NDArray[np.bool_]:\n",
    "        \"\"\"\n",
    "        Compute binary of bounding boxes in BEV space.\n",
    "        :param annotations: annotation dataclass\n",
    "        :param layers: bounding box labels to include\n",
    "        :return: binary mask as numpy array\n",
    "        \"\"\"\n",
    "        box_polygon_mask = np.zeros(self._config.bev_semantic_frame[::-1], dtype=np.uint8)\n",
    "        if annotations is not None:\n",
    "            for name_value, box_value in zip(annotations.names, annotations.boxes):\n",
    "                agent_type = tracked_object_types[name_value]\n",
    "                if agent_type in layers:\n",
    "                    # box_value = (x, y, z, length, width, height, yaw)\n",
    "                    x, y, heading = box_value[0], box_value[1], box_value[-1]\n",
    "                    box_length, box_width, box_height = (\n",
    "                        box_value[3],\n",
    "                        box_value[4],\n",
    "                        box_value[5],\n",
    "                    )\n",
    "                    agent_box = OrientedBox(StateSE2(x, y, heading), box_length, box_width, box_height)\n",
    "                    exterior = np.array(agent_box.geometry.exterior.coords).reshape((-1, 1, 2))\n",
    "                    exterior = self._coords_to_pixel(exterior)\n",
    "                    cv2.fillPoly(box_polygon_mask, [exterior], color=255)\n",
    "        # OpenCV has origin on top-left corner\n",
    "        box_polygon_mask = np.rot90(box_polygon_mask)[::-1]\n",
    "        return box_polygon_mask > 0\n",
    "\n",
    "    @staticmethod\n",
    "    def _geometry_local_coords(geometry: Any, origin: StateSE2) -> Any:\n",
    "        \"\"\"\n",
    "        Transform shapely geometry in local coordinates of origin.\n",
    "        :param geometry: shapely geometry\n",
    "        :param origin: pose dataclass\n",
    "        :return: shapely geometry\n",
    "        \"\"\"\n",
    "        a = np.cos(origin.heading)\n",
    "        b = np.sin(origin.heading)\n",
    "        d = -np.sin(origin.heading)\n",
    "        e = np.cos(origin.heading)\n",
    "        xoff = -origin.x\n",
    "        yoff = -origin.y\n",
    "\n",
    "        translated_geometry = affinity.affine_transform(geometry, [1, 0, 0, 1, xoff, yoff])\n",
    "        rotated_geometry = affinity.affine_transform(translated_geometry, [a, b, d, e, 0, 0])\n",
    "\n",
    "        return rotated_geometry\n",
    "\n",
    "    def _coords_to_pixel(self, coords):\n",
    "        \"\"\"\n",
    "        Transform local coordinates in pixel indices of BEV map.\n",
    "        :param coords: coordinates to transform\n",
    "        :return: pixel coordinates\n",
    "        \"\"\"\n",
    "        # NOTE: remove half in backward direction\n",
    "        pixel_center = np.array([[0, self._config.bev_pixel_width / 2.0]])\n",
    "        coords_idcs = (coords / self._config.bev_pixel_size) + pixel_center\n",
    "\n",
    "        return coords_idcs.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f75fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_sampling: TrajectorySampling = TrajectorySampling(time_horizon=4, interval_length=0.5)\n",
    "\n",
    "config = VADConfig()\n",
    "feature_builder = VADFeatureBuilder(config)\n",
    "target_builder = VADTargetBuilder(trajectory_sampling, config)\n",
    "\n",
    "# train_data = Dataset(\n",
    "#     scene_loader=train_scene_loader,\n",
    "#     feature_builders=agent.get_feature_builders(),\n",
    "#     target_builders=agent.get_target_builders(),\n",
    "#     cache_path=cfg.cache_path,\n",
    "#     force_cache_computation=cfg.force_cache_computation,\n",
    "# )\n",
    "\n",
    "val_data = Dataset(\n",
    "    scene_loader=val_scene_loader,\n",
    "    feature_builders=[feature_builder],\n",
    "    target_builders=[target_builder],\n",
    "    # cache_path=cfg.cache_path,\n",
    "    force_cache_computation=cfg.force_cache_computation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23754f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "def safe_collate(batch: list[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Custom collate function that safely handles strings in dicts.\n",
    "\n",
    "    Args:\n",
    "        batch (list[Dict[str, Any]]): List of samples (usually dicts) from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Collated batch where string values are left unchanged.\n",
    "    \"\"\"\n",
    "    # Handle dicts with string values safely\n",
    "    if isinstance(batch[0], dict):\n",
    "        result = {}\n",
    "        for key in batch[0]:\n",
    "            values = [d[key] for d in batch]\n",
    "            # If all values are strings, skip default_collate\n",
    "            if all(isinstance(v, str) for v in values):\n",
    "                result[key] = values\n",
    "            else:\n",
    "                result[key] = default_collate(values)\n",
    "        return result\n",
    "\n",
    "    # Fallback to default collate for non-dict batches\n",
    "    return default_collate(batch)\n",
    "\n",
    "# train_loader = DataLoader(train_data, shuffle=True, **cfg.dataloader.params)\n",
    "val_loader = DataLoader(val_data, shuffle=False, collate_fn=safe_collate, **cfg.dataloader.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "91f89dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58890054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d822edca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frame_idx',\n",
       " 'frame_sequence_length',\n",
       " 'timestamp',\n",
       " 'ego_pose',\n",
       " 'ego_velocity',\n",
       " 'ego_acceleration',\n",
       " 'ego_speed',\n",
       " 'ego_heading',\n",
       " 'ego_length',\n",
       " 'ego_width',\n",
       " 'driving_command',\n",
       " 'ego_fut_cmd',\n",
       " 'ego_his_trajs',\n",
       " 'ego_lcf_feat',\n",
       " 'cams',\n",
       " 'lidar_path',\n",
       " 'image_paths_temporal',\n",
       " 'lidar_paths_temporal',\n",
       " 'image_paths',\n",
       " 'intrinsics',\n",
       " 'cam2lidar',\n",
       " 'lidar2cam',\n",
       " 'img2lidar',\n",
       " 'lidar2img']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(batch[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c6fea89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trajectory',\n",
       " 'agent_states',\n",
       " 'agent_labels',\n",
       " 'bev_semantic_map',\n",
       " 'scene_token',\n",
       " 'log_name',\n",
       " 'map_location',\n",
       " 'timestamp',\n",
       " 'roadblock_ids',\n",
       " 'traffic_lights',\n",
       " 'gt_boxes',\n",
       " 'gt_names',\n",
       " 'gt_velocity_3d',\n",
       " 'gt_velocity',\n",
       " 'instance_tokens',\n",
       " 'track_tokens',\n",
       " 'num_agents',\n",
       " 'num_lidar_pts',\n",
       " 'num_radar_pts',\n",
       " 'valid_flag',\n",
       " 'gt_ego_fut_trajs',\n",
       " 'gt_ego_fut_masks',\n",
       " 'fut_valid_flag',\n",
       " 'gt_agent_fut_trajs',\n",
       " 'gt_agent_fut_masks',\n",
       " 'gt_agent_fut_yaw',\n",
       " 'gt_agent_lcf_feat',\n",
       " 'gt_agent_fut_goal',\n",
       " 'token',\n",
       " 'prev',\n",
       " 'next',\n",
       " 'can_bus',\n",
       " 'frame_idx',\n",
       " 'lidar2ego_translation',\n",
       " 'lidar2ego_rotation',\n",
       " 'ego2global_translation',\n",
       " 'ego2global_rotation',\n",
       " 'sweeps']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(batch[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "19698d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.3468e+00, -8.2387e-03,  9.6292e-03],\n",
       "         [ 8.6518e+00,  4.1799e-02,  2.4876e-02],\n",
       "         [ 1.2917e+01,  1.5752e-01,  3.9646e-02],\n",
       "         [ 1.7142e+01,  3.2431e-01,  5.1354e-02],\n",
       "         [ 2.1456e+01,  5.4511e-01,  5.4708e-02],\n",
       "         [ 2.5892e+01,  7.7868e-01,  5.5343e-02],\n",
       "         [ 3.0443e+01,  1.0443e+00,  5.8273e-02],\n",
       "         [ 3.5100e+01,  1.3300e+00,  6.2834e-02]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]['trajectory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01622cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3535c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74568816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d6a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24516a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d1784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95951e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9682d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d7573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27443fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87af42e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['trajectory'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "341a6c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cam_f0': ['2021.08.31.14.01.15_veh-40_00573_00681/CAM_F0/973b32a0eab4539d.jpg'],\n",
       " 'cam_l0': ['2021.08.31.14.01.15_veh-40_00573_00681/CAM_L0/a906644bb1c35de6.jpg'],\n",
       " 'cam_l1': ['2021.08.31.14.01.15_veh-40_00573_00681/CAM_L1/77c02dff4ba95faa.jpg'],\n",
       " 'cam_l2': ['2021.08.31.14.01.15_veh-40_00573_00681/CAM_L2/cf15aa6815c05ff0.jpg'],\n",
       " 'cam_r0': ['2021.08.31.14.01.15_veh-40_00573_00681/CAM_R0/6b34852138dc5781.jpg'],\n",
       " 'cam_r1': ['2021.08.31.14.01.15_veh-40_00573_00681/CAM_R1/3608bf44f1325106.jpg'],\n",
       " 'cam_r2': ['2021.08.31.14.01.15_veh-40_00573_00681/CAM_R2/46d214621c9959fc.jpg'],\n",
       " 'cam_b0': ['2021.08.31.14.01.15_veh-40_00573_00681/CAM_B0/78bf9c2b63b25251.jpg']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]['image_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f924dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021.08.31.14.01.15_veh-40_00573_00681/MergedPointCloud/69ef219183335069.pcd']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]['lidar_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722b924a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c72bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797c14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cba9130e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_paths\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m trajectory \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrajectory\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'images'"
     ]
    }
   ],
   "source": [
    "images = batch[0]['images']\n",
    "image_paths = batch[0]['image_paths']\n",
    "trajectory = batch[1]['trajectory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c1380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cam_f0', 'cam_l0', 'cam_l1', 'cam_l2', 'cam_r0', 'cam_r1', 'cam_r2', 'cam_b0'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f36700",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff7b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1080, 1920])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images['cam_f0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3f512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73d44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358255e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa8683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
